{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import reverse_geocoder as rg\n",
    "import pycountry\n",
    "import pymysql\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import logging\n",
    "import shutil\n",
    "import re\n",
    "import fiona\n",
    "import rasterio\n",
    "import fiona\n",
    "\n",
    "\n",
    "from shapely.geometry import MultiPoint , shape, mapping\n",
    "from collections import Counter\n",
    "from IPython.display import display\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from scipy.ndimage import label\n",
    "from rasterio.features import shapes\n",
    "from rasterio.plot import show\n",
    "from shapely.geometry import shape, mapping\n",
    "from scipy.ndimage import label\n",
    "from pyproj import Transformer, CRS\n",
    "\n",
    "\n",
    "from image_processing import data_preparation\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Prepation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_paths = [\"satellite_Image\"]\n",
    "output_folder_after = \"sentinel_process/Image\"\n",
    "\n",
    "data_preparation(folder_paths, output_folder_after)\n",
    "print(\"Finish process prepared data.\")\n",
    "print(\"Next stage to predict process.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    shutil.rmtree('prepare_image')\n",
    "    print(f\"Removed directory: prepare_image\")\n",
    "except FileNotFoundError:\n",
    "    pass  \n",
    "\n",
    "try:\n",
    "    shutil.rmtree('rename_image')\n",
    "    print(f\"Removed directory: rename_image\")\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "shutil.rmtree('sentinel_process')\n",
    "os.makedirs('sentinel_process')\n",
    "os.makedirs('sentinel_process/Image')\n",
    "print(f\"Created directory: sentinel_process/Image\")\n",
    "os.makedirs('sentinel_process/Raster_Burncon')\n",
    "print(f\"Created directory: sentinel_process/Raster_Burncon\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing function\n",
    "def preprocess(df, scaler_path):\n",
    "    new_column_names = [\n",
    "        'Band_3_Post', 'Band_4_Post', 'Band_5_Post', 'Band_6_Post',\n",
    "        'Band_7_Post', 'Band_8_Post', 'Band_8A_Post', 'Band_9_Post', 'Band_12_Post'\n",
    "    ]\n",
    "    rename_dict = dict(zip(df.columns[:len(new_column_names)], new_column_names))\n",
    "    df_rename = df.rename(columns=rename_dict)\n",
    "\n",
    "    with open(scaler_path, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "\n",
    "    df_normalized = scaler.transform(df_rename)\n",
    "    df_normalized = pd.DataFrame(df_normalized, columns=df_rename.columns)\n",
    "    df_rename = df_normalized.rename(columns=rename_dict)\n",
    "\n",
    "    return df_rename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prediction function\n",
    "def make_predictions(directory, df_rename):\n",
    "    loaded_model = pickle.load(open(directory, 'rb'))\n",
    "    y_pred = loaded_model.predict(df_rename)\n",
    "    df_rename[\"BURN_PREDICTED\"] = y_pred\n",
    "    label_counts = Counter(y_pred)\n",
    "    print(\"\\nCount of each predicted label:\")\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"Label {label}: {count}\")\n",
    "\n",
    "    return df_rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to create GeoTIFF from predictions\n",
    "def create_geotiff_from_predictions(predictions, original_tif_path, output_tif_path):\n",
    "    with rasterio.open(original_tif_path) as src:\n",
    "        original_metadata = src.meta.copy()\n",
    "    \n",
    "    original_metadata.update({\n",
    "        'dtype': 'uint8',\n",
    "        'count': 1,\n",
    "    })\n",
    "\n",
    "    band_1 = predictions.values.reshape((original_metadata['height'], original_metadata['width']))\n",
    "\n",
    "    with rasterio.open(output_tif_path, 'w', **original_metadata) as new_img:\n",
    "        new_img.write(band_1, 1)\n",
    "\n",
    "    print(f\"New GeoTIFF file '{output_tif_path}' has been created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the raster files\n",
    "raster_dir = r\"raster\"\n",
    "scaler_path = r\"model/min_max_scaler.pkl\"\n",
    "model_path = r\"model/Model_LGBM.sav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each file in the directory\n",
    "for file_name in os.listdir(raster_dir):\n",
    "    if file_name.endswith(\".tif\"):\n",
    "        file_path = os.path.join(raster_dir, file_name)\n",
    "\n",
    "        # Extract year from the filename using regex\n",
    "        match = re.search(r'_(\\d{4})', file_name)\n",
    "        if match:\n",
    "            year = match.group(1)\n",
    "\n",
    "            # Process the file\n",
    "            img = rasterio.open(file_path)\n",
    "            array = img.read()\n",
    "            n_bands = array.shape[0]\n",
    "\n",
    "            df = pd.DataFrame(array.reshape([n_bands, -1]).T, columns=[f\"band_{i+1}\" for i in range(n_bands)])\n",
    "            df_rename = preprocess(df, scaler_path)\n",
    "            df_predicted = make_predictions(model_path, df_rename)\n",
    "\n",
    "            # Create GeoTIFF file from predictions\n",
    "            output_tif_path = os.path.join(raster_dir, f'output_{year}.tif')\n",
    "            create_geotiff_from_predictions(df_predicted[\"BURN_PREDICTED\"], file_path, output_tif_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read the raster file\n",
    "input_raster_path = r\"raster/T47QME_20210318T035539_combined_mask.tif\"\n",
    "output_shapefile_path = r'polygon/burn_condition.shp'\n",
    "\n",
    "with rasterio.open(input_raster_path) as src:\n",
    "    raster_data = src.read(1)  # Read the first band\n",
    "    transform = src.transform  # Get the affine transform\n",
    "    crs = src.crs  # Get the CRS of the input raster\n",
    "\n",
    "    # Get the center coordinates of the raster\n",
    "    center_x = (src.bounds.left + src.bounds.right) / 2\n",
    "    center_y = (src.bounds.bottom + src.bounds.top) / 2\n",
    "\n",
    "# Create a transformer to convert from the raster's CRS to WGS84\n",
    "transformer = Transformer.from_crs(crs, \"EPSG:4326\", always_xy=True)\n",
    "\n",
    "# Convert center coordinates to WGS84\n",
    "center_lon, center_lat = transformer.transform(center_x, center_y)\n",
    "\n",
    "# Calculate the UTM zone\n",
    "utm_zone = math.floor((center_lon + 180) / 6) + 1\n",
    "hemisphere = 'north' if center_lat >= 0 else 'south'\n",
    "\n",
    "# Create a custom UTM CRS\n",
    "utm_crs = CRS.from_dict({\n",
    "    'proj': 'utm',\n",
    "    'zone': utm_zone,\n",
    "    'south': hemisphere == 'south'\n",
    "})\n",
    "\n",
    "# Create transformer from input CRS to the appropriate UTM CRS\n",
    "transformer_to_utm = Transformer.from_crs(crs, utm_crs, always_xy=True)\n",
    "\n",
    "# Use utm_crs instead of the fixed EPSG:32647\n",
    "projected_crs = utm_crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Extract features based on burn condition values\n",
    "burn_condition = (raster_data == 1).astype(np.uint8)\n",
    "\n",
    "# Label connected components\n",
    "labeled_array, num_features = label(burn_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Convert labeled features to polygons\n",
    "shapes_generator = shapes(labeled_array, transform=transform)\n",
    "\n",
    "polygons = []\n",
    "for geom, value in shapes_generator:\n",
    "    if value > 0:  # Only take the features corresponding to burn condition\n",
    "        polygons.append(shape(geom))\n",
    "\n",
    "projected_polygons = []\n",
    "for polygon in polygons:\n",
    "    # Reproject polygon to UTM\n",
    "    projected_polygon = shape(mapping(polygon))\n",
    "    projected_polygon = shape({\n",
    "        'type': 'Polygon',\n",
    "        'coordinates': [\n",
    "            [\n",
    "                transformer_to_utm.transform(x, y) for x, y in polygon.exterior.coords\n",
    "            ]\n",
    "        ]\n",
    "    })\n",
    "    projected_polygons.append(projected_polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Calculate total area and save shapefile\n",
    "total_area = sum(polygon.area for polygon in projected_polygons)\n",
    "\n",
    "schema = {\n",
    "    'geometry': 'Polygon',\n",
    "    'properties': {'id': 'int', 'area_m2': 'float'},\n",
    "}\n",
    "\n",
    "os.makedirs(os.path.dirname(output_shapefile_path), exist_ok=True)\n",
    "\n",
    "with fiona.open(output_shapefile_path, 'w', 'ESRI Shapefile', schema=schema, crs=crs) as shp:\n",
    "    for i, polygon in enumerate(projected_polygons):\n",
    "        area_m2 = polygon.area\n",
    "        shp.write({\n",
    "            'geometry': mapping(polygon),\n",
    "            'properties': {'id': i + 1, 'area_m2': area_m2},\n",
    "        })\n",
    "\n",
    "print(f\"Shapefile saved to {output_shapefile_path}\")\n",
    "print(f\"Total burn area: {total_area:.2f} square meters\\n\")\n",
    "\n",
    "# Reverse geocode to get city and province\n",
    "def get_location_info(x, y):\n",
    "    lon, lat = transformer.transform(x, y)\n",
    "    coordinates = (lat, lon)  # Note the order: (latitude, longitude)\n",
    "    result = rg.search(coordinates)\n",
    "    if result:\n",
    "        location = result[0]\n",
    "        city = location['name']\n",
    "        province = location['admin1']\n",
    "        country_code = location['cc']\n",
    "        try:\n",
    "            country = pycountry.countries.get(alpha_2=country_code).name\n",
    "        except AttributeError:\n",
    "            country = \"Unknown\"\n",
    "        return city, province, country, lat, lon\n",
    "    return None, None, None, lat, lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Print properties and plot the output\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot the original raster data\n",
    "ax.imshow(raster_data, cmap='gray', extent=(transform[2], transform[2] + transform[0] * raster_data.shape[1], \n",
    "                                            transform[5] + transform[4] * raster_data.shape[0], transform[5]))\n",
    "ax.set_title(\"Burn Condition Raster and Random Sample of Polygons\")\n",
    "\n",
    "# Determine the number of polygons to show (e.g., 5 or 10% of total, whichever is smaller)\n",
    "num_to_show = min(5, int(len(polygons) * 0.1))\n",
    "\n",
    "# Randomly select polygons to show\n",
    "polygons_to_show = random.sample(range(len(polygons)), num_to_show)\n",
    "\n",
    "# Plot the selected polygons and print their properties\n",
    "for i in polygons_to_show:\n",
    "    polygon = polygons[i]\n",
    "    x, y = polygon.exterior.xy\n",
    "    ax.plot(x, y, color='red', linewidth=2)\n",
    "    \n",
    "    # Get centroid of the polygon for reverse geocoding\n",
    "    centroid = polygon.centroid\n",
    "    city, province, country, lat, lon = get_location_info(centroid.x, centroid.y)\n",
    "    area_m2 = projected_polygons[i].area  # Get area from projected polygon\n",
    "    \n",
    "    # Annotate the polygon with its ID\n",
    "    ax.annotate(str(i+1), (centroid.x, centroid.y), color='white', fontweight='bold', ha='center', va='center')\n",
    "    \n",
    "    print(f\"Polygon {i+1}:\")\n",
    "    print(f\"  Centroid: ({lat:.6f}, {lon:.6f})\")\n",
    "    print(f\"  City: {city}, Province: {province}, Country: {country}\")\n",
    "    print(f\"  Area: {area_m2:.4f} square meters\")\n",
    "    print()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"Total number of polygons: {len(polygons)}\")\n",
    "print(f\"Number of polygons shown: {num_to_show}\")\n",
    "print(f\"Total burn area: {total_area:.2f} square meters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rida_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
